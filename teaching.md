---
layout: default
title: Teaching
permalink: /teaching/
---

<a href="javascript:history.back()" class="back-link">‚Üê back</a>

<h1> Teaching </h1>


<section id="course-overview">
  <h2>Undergraduate Course Overview</h2> 

  <p>
    This <span class="overview-highlight">undergraduate Software Engineering course</span> introduces students to 
    <span class="overview-highlight">fundamental software development practices</span> and 
    <span class="overview-highlight">team-based engineering workflows</span>. Students learn 
    <span class="overview-highlight">requirements and user stories</span>, 
    <span class="overview-highlight">UML-based system modeling</span>, and 
    <span class="overview-highlight">software architecture</span>, then apply these concepts through 
    <span class="overview-highlight">iterative implementation</span>, 
    <span class="overview-highlight">testing</span>, and 
    <span class="overview-highlight">refactoring</span>. The course emphasizes 
    <span class="overview-highlight">version control with Git</span>, 
    <span class="overview-highlight">code reviews</span>, and 
    <span class="overview-highlight">continuous integration</span>, helping students develop professional habits for building maintainable systems. 
    Through projects, students practice 
    <span class="overview-highlight">design patterns</span>, 
    <span class="overview-highlight">layered architectures</span>, and 
    <span class="overview-highlight">quality assurance</span>, gaining hands-on experience that connects core engineering principles to real-world development.
  </p>
  <a href="/teachingundergrad" class="back-link">more</a>
</section>


<section id="course-overview">
  <h2>Graduate Course Overview</h2>

  <p>
    An <span class="overview-highlight">advanced Software Engineering course</span> focuses on the construction of 
    <span class="overview-highlight">developer tools</span>, 
    <span class="overview-highlight">program analysis</span>, and the 
    <span class="overview-highlight">integration of generative AI into IDE environments</span>. 
    The curriculum grounds students in the design of 
    <span class="overview-highlight">plugin-based systems</span>, specifically building 
    <span class="overview-highlight">Visual Studio Code extensions</span> using 
    <span class="overview-highlight">TypeScript</span> and 
    <span class="overview-highlight">event-driven architectures</span>. 

    A core component involves deep 
    <span class="overview-highlight">static analysis</span>, where we utilize frameworks like 
    <span class="overview-highlight">JavaParser</span> and 
    <span class="overview-highlight">Tree-sitter</span> to implement 
    <span class="overview-highlight">Abstract Syntax Tree (AST)</span> visitors for 
    <span class="overview-highlight">structural extraction</span>, 
    <span class="overview-highlight">semantic symbol binding</span>, and 
    <span class="overview-highlight">interprocedural data flow analysis</span> (def-use chains). 

    The course culminates in 
    <span class="overview-highlight">Code Intelligence</span>, bridging these traditional techniques with 
    <span class="overview-highlight">Large Language Models (LLMs)</span> to build intelligent tools. 
    We deconstruct the 
    <span class="overview-highlight">Transformer architecture</span>, clarifying how 
    <span class="overview-highlight">Self-Attention</span> ensures syntactic consistency (e.g., maintaining variable scope) and 
    <span class="overview-highlight">Cross-Attention</span> leverages input context to generate relevant suggestions. 

    Students explore 
    <span class="overview-highlight">pre-training objectives</span> like 
    <span class="overview-highlight">MASS (Masked Sequence-to-Sequence)</span> to understand how models learn code structure from massive corpora, and apply 
    <span class="overview-highlight">fine-tuning</span> strategies to specialize models for tasks like 
    <span class="overview-highlight">code completion</span>. 

    Finally, we implement practical solutions that synchronize 
    <span class="overview-highlight">local static context</span> (extracted via ASTs) with 
    <span class="overview-highlight">remote AI inference</span>, providing hands-on experience in engineering the next generation of 
    <span class="overview-highlight">context-aware development tools</span>.
  </p>

  <p>
    In the <span class="overview-highlight">research paper discussion session</span>, students critically analyzed the evolution of 
    <span class="overview-highlight">AI-assisted software engineering</span>, tracing its development from early neural models to 
    <span class="overview-highlight">modern LLM-based agents</span>. 
    The discussion was structured around 
    <span class="overview-highlight">four major research tracks</span>. 

    First, students examined the foundations of 
    <span class="overview-highlight">neural code models</span>, focusing on the shift from general NLP to 
    <span class="overview-highlight">code-specific pre-training</span> through seminal works such as 
    <span class="overview-highlight">CodeBERT</span>, 
    <span class="overview-highlight">CodeT5</span>, and 
    <span class="overview-highlight">UniXcoder</span>. 
    These studies demonstrated how 
    <span class="overview-highlight">bimodal pre-training</span> (code and natural language) and 
    <span class="overview-highlight">AST-aware architectures</span> enable models to capture syntactic structure. 

    Second, the session explored 
    <span class="overview-highlight">code completion and generation</span>, comparing early sequence-based models with recent 
    <span class="overview-highlight">Retrieval-Augmented Generation (RAG)</span> approaches, while debating the industrial trade-offs between 
    <span class="overview-highlight">fine-tuning</span> and 
    <span class="overview-highlight">RAG</span> and considering efficiency improvements like 
    <span class="overview-highlight">memory-efficient completion</span>. 

    Third, students reviewed 
    <span class="overview-highlight">automated program repair and debugging</span>, ranging from 
    <span class="overview-highlight">test-suite-based repair</span> to 
    <span class="overview-highlight">self-planning code generation</span> where LLMs iteratively refine their outputs, alongside 
    <span class="overview-highlight">neural repair</span> using 
    <span class="overview-highlight">syntax-guided decoders</span>. 

    Finally, the discussion addressed 
    <span class="overview-highlight">security</span>, 
    <span class="overview-highlight">trust</span>, and 
    <span class="overview-highlight">evaluation</span>, analyzing risks such as 
    <span class="overview-highlight">poisoned models</span> (e.g., <em>Poisoned ChatGPT</em>), assessing 
    <span class="overview-highlight">LLM calibration</span>, and examining limitations in 
    <span class="overview-highlight">manual annotation</span> and 
    <span class="overview-highlight">code generation errors</span>, offering a realistic perspective on the challenges of 
    <span class="overview-highlight">AI-assisted engineering</span>.
  </p>

</section>
